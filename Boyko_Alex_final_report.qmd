---
title: "Predicting NFL Play Calling Tendencies"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Alex Boyko"
date: "03-13-2024"

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-aboyko22](https://github.com/stat301-2-2024-winter/final-project-2-aboyko22)

:::

## Introduction
In American Football (NFL), an offense can choose either to run or pass the ball when trying to move down the field and score. However, even when a discussion of play calling is reduced to a simple binary, it is still sufficiently difficult to understand what may happen on any given play.

Some trends have existed since the introduction of the current rules. Teams will continue to throw the ball on *3rd & Long* and run it on *4th & Inches*, but defenses know this, and part of the responsibility of calling plays is balancing playing to your strengths and following these trends with a need to go against the grain and surprise your opponent.

These decisions are designed to be unpredictable. So, that raises the question — how accurately can we actually guess whether a team will run or pass? That is the goal of this project, to understand if predictive modeling can understand trends and patterns within this variability and see just how accurately it can predict play calls.

## Data Overview
The data for this project comes from `nflverse`, an R package built around play-by-play data. For this project, data from 2022 was used to perform an exploratory data analysis on the set of variables and make decisions about which may have the greatest impact of predicting play calls.

Within this data set, `play_type` is the target variable, a factor variable that has been filtered out to be either 'run' or 'pass'. The `nflverse` package does an incredible job at keeping data clean, so missingness is not an issue. The only transformation that needs to occur is to remove observations from the data that are not runs or passes, such as special team plays and kneeldowns.

During the initial EDA, the `play_type` variable was examined to ensure that there would be no future issues caused by a severe skewness in the observations, which is split *58.5%* pass and *41.5%* runs.

```{r, echo=FALSE}
library(tidyverse)
library(here)

```

## Methods

::: {.callout-tip}

*Should cover the data splitting procedure and clearly identify what type of prediction problem it is. State and describe the model types you will be fitting. Describe any parameters that will be tuned. Describe what recipes will be used. Describe the resampling technique used. In some cases an extended discussion about recipe variations might be useful. Especially if students are using recipe variation to try and explore the predictive importance of certain variables. Explain the metric that will be used to compare and ultimately used to select a final model.*

:::

Once the data was cleaned and prepared, more on that below, the data was initially split into an 80-20 proportion of training and testing sets, respectively. Resamples were then created from the training set using the cross-validation v-folding function `vfold_cv`, with four repeats and 10 partitions generating 40 different folds of the data. These folds were then used to fit the following model types.

All in all, 31,825 total plays were split into a training set of 25,459 and a testing set of 6,366. These plays exclusively come from the regular season the 2023 NFL campaign, and excludes Week 18, as this is a time where many team decide to rest their starters and significantly alter the way they play.

### Recipes
Four recipes types were used, corresponding to different levels of information.

#### The Null
This recipe is pretty self-explanatory. Without any features, the most accurate prediction we can make is to see if runs or passes happen more often, and simply always choose the more-frequent one. In this case, that would mean always predicting a pass.

#### The Casual Fan
Any information used in this recipe must be visible on a TV broadcast. These include predictors like down and distance, field positioning, quarter number, and score differential.

These two recipes, once fit to their respective models, are intended to serve as baselines. It is not expected that these will perform as well as the following recipes, but serve an important purpose as they help determine the effectiveness of the other models and will maintain information relevant to potential questions of inference later on.

#### The Coach
This sets a baseline for what I believe to a reasonable accuracy rate for a person predicting plays just before they happen without any external aid. The recipe attempts to account for trends that a coach would know off-hand, like offensive 3rd down efficiency, goal-to-go scenarios, the last play that was ran, and other important external conditions. Any highly specific numbers and figures are not expected to be known.

#### The Computer
With all of the intricate details of the data set, there are a lot of exact measurements that can be used that a person may be able to closely intuit, but not fully know. This recipe uses all the information the coach recipe has, but also includes rolling success rates throughout the game and previous play-calling patterns, pre-snap probabilities of various drive outcomes and overall win likelihood, and season trends.

Both of these recipes are still much more simple than what factors into real decision making, but they are limited both intentionally and unintentionally by the constraints of the data set. These recipes have both alternate versions where categorical variables are dummied or not dummied so they can be used on the following model types.

### Models
Aside from the null model specification, courtesy of `parsnip`, four other model types were used.

#### Logistic Regression
This model type was used only for the casual fan baseline. The specific engine is `glmnet` and the model was run with a 0.01 penalty value. No hyperparameters were tuned for this model. 

#### K Nearest Neighbors
The k-nearest numbers engine is `kknn`. The `neighbors` hyperparameter was tuned across a range of 10 to 100 across 10 levels.

#### Boosted Tree
The boosted tree engine used was `xgboost`, and all three of `min_n`, `mtry`, and `learn_rate` were tuned. Five levels were used for each, with the default range used for `min_n`, a range of 1 to 8 or 15 for `mtry`, since the two recipes have different numbers of total variable used, and a range of -2 to -0.2 for the learn rate.

#### Random Forest
Finally, the random forest models were created using the `ranger` engine, and the `min_n` and `mtry` parameters were once again tuned, with `min_n` ranges of 2 to 10 and 4 to 18 and an `mtry` range of 4 to 20.

The results of all of these models were tabulated and judged in effectiveness using the accuracy metric and its standard error.

## Model Building & Selection

Here are the results from the initial model tunings and fits onto the resamples. These averages and standard errors were computed using the 40 total folds mentioned above.

```{r, echo=FALSE}
load(here("results/best_models.rda"))
best_models %>% knitr::kable()

```

This is what these results look like in plot form, with point estimates and error bars stretching one standard error above and below it.

```{r, echo=FALSE}
knitr::include_graphics("plots/accuracy_plot.jpg")

```

Overall, these results look as expected given the amount of information provided in the individual recipes and the flexibility of the model types. The casual fan baseline model performed much better than the null model, increasing its accuracy by nearly 6% by only using 5 predictors. The KNN models performed even better, but the extremely flexible boosted tree and random forest models, which were also tuned the most, were in a category of their own.

Similarly, there was a consistent difference between the performance of the models using the coach and computer recipes, although the gap is not as large as expected. With the standard errors all being remarkably low, this is significant, but I anticipated the recipes playing a much larger role than model specifications in accuracy results.

One interesting artifact exists, with the best boosted tree model performing better than the best random forest one using the coach recipe type, but worse with the computer type. This is likely due to the increased flexibility of the random forest model engine, and with the coach recipe having significantly fewer predictors, this may have allowed it to hyperfocus on unimportant noise to a greater extent.

That being said, looking at these results, the random forest with the computer recipe type performed the best. This is not surprising to me, since it took by far the longest to compute at nearly 10 hours, and its best performing set of hyperparamters will be used going forward.

## Final Model Analysis

::: {.callout-tip}

*This is where you fit your final/winning model to the testing data. Assess the final model’s performance with at least the metric used to determine the winning model, but it is also advisable to use other performance metrics (especially ones that might be easier to communicate/understand). Should include an exploration of predictions vs the true values (graph) or a confusion matrix (table). Remember to consider the scale of your outcome variable at this time — did you transform the target variable? If a transformation was used, then you should consider conducting analyses on both the original and transformed scale of the target variable. Is the model any good? It might be the best of the models you tried, but does the effort of building a predictive model really pay off — is it that much better than a baseline/null model? Were there any features of the model you selected that make it the best (e.g. fits nonlinearity well)?*

:::

So, the workflow from the best performing random forest model using the `computer` recipe type, which had a `mtry` parameter of 8 and a `min_n` of 20, was used to fit the final model.

```{r, echo=FALSE}
load(here("results/final_model_results.rda"))
final_model_results %>% knitr::kable()
```


```{r, echo=FALSE}
knitr::include_graphics(here("plots/faceted_plot.jpg"))
```

```{r, echo=FALSE}
load(here("results/faceted_table.rda"))
faceted_table %>% knitr::kable()
```

Results from `nflverse` model

```{r, echo=FALSE}
load(here("results/xpass_final_results.rda"))
xpass_final_results %>% knitr::kable()
```

## Conclusion

::: {.callout-tip}

*State any conclusions or discoveries/insights. This is a great place for future work, new research questions, and next steps.*

:::

```{r, echo=FALSE}
knitr::include_graphics(here("plots/texans_graph.jpg"))
```

## References

All data used in this project is coming from the `nflverse package`. I’ve provided links to the [repository](https://github.com/nflverse) and [documentation](https://nflverse.nflverse.com/) to show how this data is collected and updated.

