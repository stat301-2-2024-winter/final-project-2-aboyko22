---
title: "Predicting NFL Play Calling Tendencies"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Alex Boyko"
date: "03-13-2024"

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-aboyko22](https://github.com/stat301-2-2024-winter/final-project-2-aboyko22)

:::

## Introduction
In American Football (NFL), an offense can choose either to run or pass the ball when trying to move down the field and score. However, even when a discussion of play calling is reduced to a simple binary, it is still sufficiently difficult to understand what may happen on any given play.

Some trends have existed since the introduction of the current rules. Teams will continue to throw the ball on *3rd & Long* and run it on *4th & Inches*, but defenses know this, and part of the responsibility of calling plays is balancing playing to your strengths and following these trends with a need to go against the grain and surprise your opponent.

These decisions are designed to be unpredictable. So, that raises the question — how accurately can we actually guess whether a team will run or pass? That is the goal of this project, to understand if predictive modeling can understand trends and patterns within this variability and see just how accurately it can predict play calls.

## Data Overview
The data for this project comes from `nflverse`, an R package built around play-by-play data. For this project, data from 2022 was used to perform an exploratory data analysis on the set of variables and make decisions about which may have the greatest impact of predicting play calls.

Within this data set, `play_type` is the target variable, a factor variable that has been filtered out to be either 'run' or 'pass'. The `nflverse` package does an incredible job at keeping data clean, so missingness is not an issue. The only transformation that needs to occur is to remove observations from the data that are not runs or passes, such as special team plays and kneeldowns.

During the initial EDA, the `play_type` variable was examined to ensure that there would be no future issues caused by a severe skewness in the observations, which is split *58.5%* pass and *41.5%* runs.

```{r, echo=FALSE}
library(tidyverse)
library(here)

```

## Methods
*Should cover the data splitting procedure and clearly identify what type of prediction problem it is. State and describe the model types you will be fitting. Describe any parameters that will be tuned. Describe what recipes will be used. Describe the resampling technique used. In some cases an extended discussion about recipe variations might be useful. Especially if students are using recipe variation to try and explore the predictive importance of certain variables. Explain the metric that will be used to compare and ultimately used to select a final model.*

Once the data was cleaned and prepared, more on that below, the data was initially split into an 80-20 proportion of training and testing sets, respectively. Resamples were then created from the training set using the cross-validation v-folding function `vfold_cv`, with four repeats and 10 partitions generating 40 different folds of the data. These folds were then used to fit the following model types.

### Recipes
Four recipes types were used, corresponding to different levels of information

#### The Null
This recipe is pretty self-explanatory. Without any features, the most accurate prediction we can make is to see if runs or passes happen more often, and simply always choose the more-frequent one.

#### The Casual Fan
I set this aside with the intention of these being at about the same level as a new or casual football fan. Any information at their disposal must be visible on a TV broadcast. These include predictors like down and distance, field positioning, time, and score.

These two recipes, once fit to their respective models, are intended to serve as baselines. It is not expected that these will perform as well as the following recipes, but serve an important purpose as they help determine the effectiveness of the other models and will maintain information relevant to potential questions of inference later on.

#### The Coach
I set this aside with the intention of these being at about the same level as a new or casual football fan. Any information at their disposal must be visible on a TV broadcast. These include predictors like down and distance, field positioning, time, and score.

#### The Computer
I set this aside with the intention of these being at about the same level as a new or casual football fan. Any information at their disposal must be visible on a TV broadcast. These include predictors like down and distance, field positioning, time, and score.

### Models
Aside from the null model specification, courtesy of `parsnip`, four other model types were used.

#### Logistic Regression
Tunes: none
#### K Nearest Neighbors
Tunes: neighbors
#### Boosted Tree
Tunes: min_n, mtry, and learn rate
#### Random Forest
Tunes: mtry and min_n

## Model Building & Selection
*Should reiterate the metric that will be used to compare models and determine which will be the final/winning model. Include a table of the best performing model results. Review and analysis of tuning parameters should happen here. Should further tuning be explored? Or how should tuning be adjusted when fitting data like this in the future. This would be a good section to describe what the best parameters were for each model type. Could include a discussion comparing any systematic differences in performance between model types or recipes. If variations in recipes were used to explore predictive importance of certain variables, then it should be discussed here. The section will likely end with the selection of the final/winning model (provide your reasoning). Was it surprising or not surprising that this particular model won? Explain.*

```{r, echo=FALSE}
load(here("results/best_models.rda"))

best_models %>% knitr::kable()
knitr::include_graphics("plots/accuracy_plot.jpg")
```


## Final Model Analysis
*This is where you fit your final/winning model to the testing data. Assess the final model’s performance with at least the metric used to determine the winning model, but it is also advisable to use other performance metrics (especially ones that might be easier to communicate/understand). Should include an exploration of predictions vs the true values (graph) or a confusion matrix (table). Remember to consider the scale of your outcome variable at this time — did you transform the target variable? If a transformation was used, then you should consider conducting analyses on both the original and transformed scale of the target variable. Is the model any good? It might be the best of the models you tried, but does the effort of building a predictive model really pay off — is it that much better than a baseline/null model? Were there any features of the model you selected that make it the best (e.g. fits nonlinearity well)?*

## Conclusion
*State any conclusions or discoveries/insights. This is a great place for future work, new research questions, and next steps.*

## References
*Any references used should be sited here. This includes but is not limited to where you got your data. There is no “formal” reference guideline but we recommend APA format. Example: Lastname, F. M. (Year, Month Date). Title of page. Site name. URL*